{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e2babf4",
   "metadata": {
    "id": "3e2babf4"
   },
   "source": [
    "<center>\n",
    "    <u><h2>Computer Vision With OpenCV</h2></u>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04313fe4",
   "metadata": {
    "executionInfo": {
     "elapsed": 845,
     "status": "ok",
     "timestamp": 1682357110491,
     "user": {
      "displayName": "APnML Batch01",
      "userId": "07521146791791204481"
     },
     "user_tz": -330
    },
    "id": "04313fe4"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8b37a41",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1682357112217,
     "user": {
      "displayName": "APnML Batch01",
      "userId": "07521146791791204481"
     },
     "user_tz": -330
    },
    "id": "e8b37a41"
   },
   "outputs": [],
   "source": [
    "def show_image(image, title=\"LIVE\"):\n",
    "    while True:\n",
    "        cv2.imshow(title, image)\n",
    "\n",
    "        k = cv2.waitKey(1)\n",
    "        if k == 27:\n",
    "            break\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ec2bbbc",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1682357113734,
     "user": {
      "displayName": "APnML Batch01",
      "userId": "07521146791791204481"
     },
     "user_tz": -330
    },
    "id": "2ec2bbbc"
   },
   "outputs": [],
   "source": [
    "def convert(image, from_to=cv2.COLOR_BGR2RGB):\n",
    "    converted = cv2.cvtColor(image, from_to)\n",
    "    return converted\n",
    "\n",
    "## Reading an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08ef4a44",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1682357115732,
     "user": {
      "displayName": "APnML Batch01",
      "userId": "07521146791791204481"
     },
     "user_tz": -330
    },
    "id": "08ef4a44"
   },
   "outputs": [],
   "source": [
    "def load_image(path=\"sources/people.jpg\"):\n",
    "    global image\n",
    "    image = cv2.imread(path)\n",
    "load_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DZfsf_WeTSzj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "error",
     "timestamp": 1682357205560,
     "user": {
      "displayName": "APnML Batch01",
      "userId": "07521146791791204481"
     },
     "user_tz": -330
    },
    "id": "DZfsf_WeTSzj",
    "outputId": "5d8b13e7-db23-4264-f32c-846190ac67a6"
   },
   "outputs": [],
   "source": [
    "load_image()\n",
    "show_image(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cebf810",
   "metadata": {
    "id": "5cebf810"
   },
   "source": [
    "<a href=\"https://stackoverflow.com/questions/36218385/parameters-of-detectmultiscale-in-opencv-using-python\"><i>refer here for cascade classifier's detect multi scale method's parameters</i></a><br>\n",
    "<a href=\"https://www.codesofinterest.com/2017/07/more-fonts-on-opencv.html\"><i>refer here for opencv fonts</i></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8c4e4e",
   "metadata": {
    "id": "fc8c4e4e"
   },
   "source": [
    "<hr>\n",
    "<h2>Spliting and merging image color channels</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ae12b6",
   "metadata": {
    "id": "83ae12b6",
    "outputId": "e19b79b9-a1ea-47ad-f864-42c401d7677d"
   },
   "outputs": [],
   "source": [
    "b, g, r = cv2.split(image)\n",
    "\n",
    "print(b.shape)\n",
    "print(g.shape)\n",
    "print(r.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8330431",
   "metadata": {
    "id": "b8330431"
   },
   "outputs": [],
   "source": [
    "new_image = cv2.merge(( b, g, r ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581d87bb",
   "metadata": {
    "id": "581d87bb"
   },
   "outputs": [],
   "source": [
    "show_image(new_image, \"NEW-IMAGE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5daccca",
   "metadata": {
    "id": "b5daccca"
   },
   "source": [
    "Note: The cv2.split() function is a slow function. Numpy indexing is quit efficient and it should be used if possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c3e450",
   "metadata": {
    "id": "95c3e450"
   },
   "source": [
    "<hr>\n",
    "<h2>Draw borders on an image</h2>\n",
    "<i>cv2.copyMakeBorder(src,top,bottom,left,right,border type)</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65394ba0",
   "metadata": {
    "id": "65394ba0"
   },
   "outputs": [],
   "source": [
    "replicated = cv2.copyMakeBorder(image, 10, 10, 10, 10, cv2.BORDER_REPLICATE)\n",
    "reflected = cv2.copyMakeBorder(image, 10, 10, 10, 10, cv2.BORDER_REFLECT)\n",
    "reflected_101 = cv2.copyMakeBorder(image, 10, 10, 10, 10, cv2.BORDER_REFLECT_101)\n",
    "constant = cv2.copyMakeBorder(image, 10, 10, 10, 10, cv2.BORDER_CONSTANT, value=(255, 0, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c055da76",
   "metadata": {
    "id": "c055da76",
    "outputId": "d37324be-5169-4d85-84a7-b835a2d07694"
   },
   "outputs": [],
   "source": [
    "plt.imshow(convert(replicated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281cc22a",
   "metadata": {
    "id": "281cc22a",
    "outputId": "4a88d461-dcc4-49bb-d17a-35a28920c0d4"
   },
   "outputs": [],
   "source": [
    "plt.imshow(convert(reflected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f85e6db",
   "metadata": {
    "id": "1f85e6db",
    "outputId": "426ad2df-413e-42b2-ff47-b0e872d44ae1"
   },
   "outputs": [],
   "source": [
    "plt.imshow(convert(reflected_101))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b913a212",
   "metadata": {
    "id": "b913a212",
    "outputId": "e26798e0-b7e0-44fe-e4c8-f28bce82ce72"
   },
   "outputs": [],
   "source": [
    "plt.imshow(convert(constant))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f7b2c2",
   "metadata": {
    "id": "07f7b2c2"
   },
   "source": [
    "<hr>\n",
    "<h2>Image Resizing</h2>\n",
    "<i>cv2.resize(src, dsize[, dst[, fx[,fy[,interpolation]]])</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ec614d",
   "metadata": {
    "id": "24ec614d"
   },
   "outputs": [],
   "source": [
    "inter_nearest = cv2.resize(image, (1000, 1000), cv2.INTER_NEAREST)\n",
    "inter_area = cv2.resize(image, (1000, 1000), cv2.INTER_AREA)\n",
    "inter_cubic = cv2.resize(image, (1000, 1000), cv2.INTER_CUBIC)\n",
    "inter_lanczos4 = cv2.resize(image, (1000, 1000), cv2.INTER_LANCZOS4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad853e5b",
   "metadata": {
    "id": "ad853e5b",
    "outputId": "05df790e-8c4e-4f0f-a2dd-3fb6228ff4fb"
   },
   "outputs": [],
   "source": [
    "plt.imshow(convert(inter_nearest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73152083",
   "metadata": {
    "id": "73152083",
    "outputId": "91c56a24-cf3a-429c-b33b-71878a3cac82"
   },
   "outputs": [],
   "source": [
    "plt.imshow(convert(inter_area))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cff646e",
   "metadata": {
    "id": "8cff646e",
    "outputId": "de12f8d2-8150-4e94-f0aa-e769ca2a1784"
   },
   "outputs": [],
   "source": [
    "plt.imshow(convert(inter_cubic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc497734",
   "metadata": {
    "id": "cc497734",
    "outputId": "75bc64e1-d66d-4cd0-c018-44a3a745b36e"
   },
   "outputs": [],
   "source": [
    "plt.imshow(convert(inter_lanczos4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7abde7e",
   "metadata": {
    "id": "b7abde7e"
   },
   "source": [
    "<i>If you are enlarging the image, you should prefer to use INTER_LINEAR or INTER_CUBIC interpolation. If you are shrinking the image, you should prefer to use INTER_AREA interpolation.\n",
    "\n",
    "Cubic interpolation is computationally more complex, and hence slower than linear interpolation. However, the quality of the resulting image will be higher.</i><br>\n",
    "<a href=\"https://theailearner.com/2018/11/15/image-interpolation-using-opencv-python/\"><i>for more about interpolation</i></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcc1c8f",
   "metadata": {
    "id": "6bcc1c8f"
   },
   "outputs": [],
   "source": [
    "rate = 0.4 #(4%)\n",
    "dimension = int(image.shape[0] * (rate + 0.2)), int(image.shape[1] * rate)\n",
    "resized = cv2.resize(image, dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed13af8b",
   "metadata": {
    "id": "ed13af8b"
   },
   "outputs": [],
   "source": [
    "show_image(resized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97094c2",
   "metadata": {
    "id": "d97094c2"
   },
   "source": [
    "<hr>\n",
    "<h2>Image Rotation</h2>\n",
    "<i>cv2.getRotationMatrix2D(center, angle, scale, rotated = cv2.warpAfifne(img,M,(w,h))  </i>\n",
    "<br>\n",
    "<p>The image can be rotated in various angles (90,180,270 and 360). OpenCV calculates the affine matrix that performs affine transformation, which means it does not preserve the angle between the lines or distances between the points, although it preserves the ratio of distances between points lying on the lines.</p>\n",
    "<br>\n",
    "<li>center: It represents the center of the image.</li>\n",
    "<li>angle: It represents the angle by which a particular image to be rotated in the anti-clockwise direction.</li>\n",
    "<li>rotated: ndarray that holds the rotated image data.</li>\n",
    "<li>scale: The value 1.0 is denoted that the shape is preserved. Scale the image according to the provided value.</li>\n",
    "<br><br>\n",
    "<a href=\"https://medium.com/mlait/affine-transformation-image-processing-in-tensorflow-part-1-df96256928a\"><i>For more about affine transformation<i></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a190ce3",
   "metadata": {
    "id": "3a190ce3"
   },
   "outputs": [],
   "source": [
    "center = image.shape[0] / 2, image.shape[1] / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903be271",
   "metadata": {
    "id": "903be271"
   },
   "outputs": [],
   "source": [
    "rotationMatrix2D = cv2.getRotationMatrix2D(center, 90, 0.6)\n",
    "rotated = cv2.warpAffine(image, rotationMatrix2D, (image.shape[0], image.shape[1]))\n",
    "show_image(rotated, \"ROTATED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd1e1ec",
   "metadata": {
    "id": "3bd1e1ec"
   },
   "source": [
    "<hr>\n",
    "<h2>Drawing Shapes</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55071001",
   "metadata": {
    "id": "55071001"
   },
   "source": [
    "<li>Circle: <i>cv2.circle(img, center, radius, color[,thickness [, lineType[,shift]]])</i></li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d930e9e5",
   "metadata": {
    "id": "d930e9e5",
    "outputId": "ce1e6dac-1698-4004-dea1-21f711102744"
   },
   "outputs": [],
   "source": [
    "cv2.circle(image, (270, 114), 50, (0, 0, 255), 2)\n",
    "plt.imshow(convert(image))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a425647",
   "metadata": {
    "id": "6a425647"
   },
   "source": [
    "<li>Rectangle: <i>cv2.rectangle(img, (x, y), (x+w, y+h), color[, thickness[,lineType[,shift]]])</i></li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2bcc78",
   "metadata": {
    "id": "db2bcc78",
    "outputId": "171bb011-0b29-48f5-91eb-7c3ec1ceebd4"
   },
   "outputs": [],
   "source": [
    "cv2.rectangle(image, (233, 68), (310, 144), (0, 255, 0), 2)\n",
    "plt.imshow(convert(image))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330e1132",
   "metadata": {
    "id": "330e1132"
   },
   "source": [
    "<li>Eclipse: <i>cv2.ellipse(img, center, axes length, angle, startAngle, endAngle, color[, thickness[, lineType[, shift]]]), cv2.ellipse(img, box, color[, thickness[, lineType]])</i>\n",
    "    \n",
    "<a href=\"https://www.geeksforgeeks.org/python-opencv-cv2-ellipse-method/\"><i>for further details</i></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265d7074",
   "metadata": {
    "id": "265d7074",
    "outputId": "508a1797-c238-43a4-dd7e-2ec657932b64"
   },
   "outputs": [],
   "source": [
    "cv2.ellipse(image, (413, 93), (100, 50), 0, 0, 360, (0, 255, 0), 2)\n",
    "plt.imshow(convert(image))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f480ce6",
   "metadata": {
    "id": "9f480ce6"
   },
   "source": [
    "<li>Line: <i>cv2.line(img, pt1, pt2, color[, thickness[, lineType[, shift]]])</i></li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861b765e",
   "metadata": {
    "id": "861b765e",
    "outputId": "d7504edf-f1e3-4c87-ccc8-878bbe6efb3e"
   },
   "outputs": [],
   "source": [
    "cv2.line(image, (11, 179), (600, 181), (0, 0, 255), 2)\n",
    "plt.imshow(convert(image))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696d7271",
   "metadata": {
    "id": "696d7271"
   },
   "source": [
    "<li>Text: <i>cv2.putText(img, text, org, font, font size, color)</i></li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4915611",
   "metadata": {
    "id": "a4915611",
    "outputId": "5983e4f8-8616-4426-8f06-b30a1cf86c98"
   },
   "outputs": [],
   "source": [
    "cv2.putText(image, \"WE ARE THE WORLD\", (150, 30), cv2.FONT_HERSHEY_COMPLEX, 1, (255, 0, 0))\n",
    "plt.imshow(convert(image))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a286f87a",
   "metadata": {
    "id": "a286f87a"
   },
   "source": [
    "<hr>\n",
    "<h2>Oriented FAST and Rotated BRIEF - ORB</h2>\n",
    "<a href=\"https://docs.opencv.org/4.5.2/d1/d89/tutorial_py_orb.html\"><i>for more information</i></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659333bf",
   "metadata": {
    "id": "659333bf"
   },
   "outputs": [],
   "source": [
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Set up the orb detector.\n",
    "detector = cv2.ORB_create()\n",
    "\n",
    "# Detect blobs from the image\n",
    "keypoints = detector.detect(gray, None)\n",
    "\n",
    "# cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS - \n",
    "# This method draws detected blobs as green circles and ensures that the size of the circle corresponds \n",
    "# to the size of the blob.\n",
    "img_keypoints = cv2.drawKeypoints(image, keypoints, None, (0, 255, 0))\n",
    "\n",
    "show_image(img_keypoints, \"ORB KEYPOINTS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b9700c",
   "metadata": {
    "id": "e2b9700c"
   },
   "source": [
    "<hr>\n",
    "<h2>Background Subtraction</h2>\n",
    "<a href=\"https://docs.opencv.org/3.4/d1/dc5/tutorial_background_subtraction.html\"><i>for more information</i></a><br>\n",
    "<a href=\"https://www.geeksforgeeks.org/background-subtraction-opencv/\"><i>here also available</i></a><br>\n",
    "<i>install: pip install opencv-contrib-python</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696b0387",
   "metadata": {
    "id": "696b0387"
   },
   "outputs": [],
   "source": [
    "mog = cv2.bgsegm.createBackgroundSubtractorMOG()\n",
    "mog2 = cv2.createBackgroundSubtractorMOG2()\n",
    "gmg = cv2.bgsegm.createBackgroundSubtractorGMG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074b7302",
   "metadata": {
    "id": "074b7302"
   },
   "outputs": [],
   "source": [
    "source = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = source.read()\n",
    "    \n",
    "    if ret:\n",
    "        mog_img = mog.apply(frame)\n",
    "        mog2_img = mog2.apply(frame)\n",
    "        gmg_img = gmg.apply(frame)\n",
    "        \n",
    "        cv2.imshow(\"MOG\", mog_img)\n",
    "        cv2.imshow(\"MOG2\", mog2_img)\n",
    "        cv2.imshow(\"GMG\", gmg_img)\n",
    "        \n",
    "        if cv2.waitKey(1) == 27:\n",
    "            break\n",
    "source.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b232bc91",
   "metadata": {
    "id": "b232bc91",
    "outputId": "6c0a0a8e-4abe-4a3b-b5b7-fbeefec506f0"
   },
   "outputs": [],
   "source": [
    "plt.subplot(131)\n",
    "plt.title(\"MOG\")\n",
    "plt.imshow(mog_img, cmap=\"gray\")\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.title(\"MOG-2\")\n",
    "plt.imshow(mog2_img, cmap=\"gray\")\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.title(\"GMG\")\n",
    "plt.imshow(gmg_img, cmap=\"gray\")\n",
    "plt.xticks([])\n",
    "plt.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a91a0d",
   "metadata": {
    "id": "98a91a0d"
   },
   "source": [
    "<i>Note that from above results GMG subtractor gives result in higher rate of noise. It is recommended that whenever work with GMG there should be remove noises by using morphological transformations. </i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451f81af",
   "metadata": {
    "id": "451f81af"
   },
   "outputs": [],
   "source": [
    "source = cv2.VideoCapture(0)\n",
    "kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (9, 9))\n",
    "\n",
    "while True:\n",
    "    ret, frame = source.read()\n",
    "    \n",
    "    if ret:\n",
    "        gmg_img = gmg.apply(frame)\n",
    "        denoised_gmg_img = cv2.morphologyEx(gmg_img, cv2.MORPH_OPEN, kernel)\n",
    "        \n",
    "        cv2.imshow(\"GMG\", gmg_img)\n",
    "        cv2.imshow(\"DENOISED GMG\", denoised_gmg_img)\n",
    "        if cv2.waitKey(1) == 27:\n",
    "            break\n",
    "source.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9b3209",
   "metadata": {
    "id": "3f9b3209",
    "outputId": "eeb42038-928f-4645-cd04-54392ac4ad29"
   },
   "outputs": [],
   "source": [
    "plt.subplot(121)\n",
    "plt.title(\"GMG\")\n",
    "plt.imshow(gmg_img, cmap=\"gray\")\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(\"GMG (denoised)\")\n",
    "plt.imshow(denoised_gmg_img, cmap=\"gray\")\n",
    "plt.xticks([])\n",
    "plt.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6085315d",
   "metadata": {
    "id": "6085315d"
   },
   "source": [
    "<hr>\n",
    "<h2>Edge Detection</h2>\n",
    "<a href=\"https://docs.opencv.org/3.4/da/d22/tutorial_py_canny.html\"><i>for official documentation</i></a><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5d35aa",
   "metadata": {
    "id": "9e5d35aa",
    "outputId": "7f268d16-9f8f-4c3b-c8eb-2aa6e0750b3d"
   },
   "outputs": [],
   "source": [
    "edges = cv2.Canny(image, 100, 250, L2gradient=True)\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title(\"real\")\n",
    "plt.imshow(convert(image))\n",
    "plt.subplot(122)\n",
    "plt.title(\"edges\")\n",
    "plt.imshow(edges, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349abef5",
   "metadata": {
    "id": "349abef5"
   },
   "source": [
    "<hr>\n",
    "<h2>Image Smoothing(Bluring)</h2><br>\n",
    "<h3><u>Pixel Averaging</u></h3>\n",
    "<p>In this technique, the image is convolved with a box filter (normalize). It calculates the average of all the pixels which are under the kernel area and replaces the central element with the calculated average. OpenCV provides the cv2.blur() or cv2.boxFilter() to perform this operation. We should define the width and height of the kernel. The syntax of cv2.blur() function is following.</p>\n",
    "<i>cv2.blur(src, dst, ksize, anchor, borderType)</i><br>\n",
    "\n",
    "<li>src: It represents the source (input) image.</li>\n",
    "<li>dst: It represents the destination (output) image.</li>\n",
    "<li>ksize: It represents the size of the kernel.</li>\n",
    "<li>anchor: It denotes the anchor points.</li>\n",
    "<li>Border type: It represents the type of border to be used to the output.</li>\n",
    "<br>\n",
    "<a href=\"https://docs.opencv.org/3.4/dc/dd3/tutorial_gausian_median_blur_bilateral_filter.html\"><i>\n",
    "    official doc for filters</i></a><br>\n",
    "<a href=\"https://docs.opencv.org/4.5.2/d4/d13/tutorial_py_filtering.html\"><i>\n",
    "    official doc for image smoothing</i></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7bd137",
   "metadata": {
    "id": "bc7bd137"
   },
   "source": [
    "<h4>2D Convolutionizing for image smoothing</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f4484b",
   "metadata": {
    "id": "c3f4484b",
    "outputId": "e8101735-8091-4675-d05a-73ab71c592e4"
   },
   "outputs": [],
   "source": [
    "kernel = np.ones((7, 7)) * (1 / (7 * 7))\n",
    "blured = cv2.filter2D(image, -1, kernel)\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title(\"real\")\n",
    "plt.imshow(convert(image), aspect=\"auto\")\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.subplot(122)\n",
    "plt.title(\"blured\")\n",
    "plt.imshow(convert(blured), aspect=\"auto\")\n",
    "plt.xticks([])\n",
    "plt.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb527f1",
   "metadata": {
    "id": "4cb527f1"
   },
   "source": [
    "<ol  start=\"1\"><li>Averaging</li></ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf16f39",
   "metadata": {
    "id": "eaf16f39",
    "outputId": "c21166b3-c99c-4b90-af78-8c023ab8a273"
   },
   "outputs": [],
   "source": [
    "blured = cv2.blur(image, (7, 7))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title(\"real\")\n",
    "plt.imshow(convert(image), aspect=\"auto\")\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.subplot(122)\n",
    "plt.title(\"blured\")\n",
    "plt.imshow(convert(blured), aspect=\"auto\")\n",
    "plt.xticks([])\n",
    "plt.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8d6ed9",
   "metadata": {
    "id": "3a8d6ed9"
   },
   "source": [
    "<ol  start=\"2\"><li>Gaussian Bluring</li></ol>\n",
    "<i>cv2.GaussianBlur(src, (x, y), (sigma-x, sigma-y)</i><br><br>\n",
    "In this method, instead of a box filter, a Gaussian kernel is used. It is done with the function, cv.GaussianBlur(). We should specify the width and height of the kernel which should be positive and odd. We also should specify the standard deviation in the X and Y directions, sigmaX and sigmaY respectively. If only sigmaX is specified, sigmaY is taken as the same as sigmaX. If both are given as zeros, they are calculated from the kernel size. Gaussian blurring is highly effective in removing Gaussian noise from an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f693ca7",
   "metadata": {
    "id": "3f693ca7",
    "outputId": "799afa6c-4fae-4761-ff2d-f0a1c40ccee5"
   },
   "outputs": [],
   "source": [
    "load_image(\"sources/omg.webp\")\n",
    "blured = cv2.GaussianBlur(image, (17, 17), 0)\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title(\"real\")\n",
    "plt.imshow(convert(image), aspect=\"auto\")\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.subplot(122)\n",
    "plt.title(\"blured\")\n",
    "plt.imshow(convert(blured), aspect=\"auto\")\n",
    "plt.xticks([])\n",
    "plt.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db36f636",
   "metadata": {
    "id": "db36f636"
   },
   "source": [
    "<ol  start=\"3\"><li>Median Bluring</li></ol>\n",
    "<i>cv2.MedianBlur(src, kernel)</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07ad288",
   "metadata": {
    "id": "b07ad288",
    "outputId": "7a111568-ee05-4412-d40b-ee290fdc39b8"
   },
   "outputs": [],
   "source": [
    "load_image(\"sources/chaplin.png\")\n",
    "blured = cv2.medianBlur(image, 3)\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title(\"real\")\n",
    "plt.imshow(convert(image), aspect=\"auto\")\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.subplot(122)\n",
    "plt.title(\"blured\")\n",
    "plt.imshow(convert(blured), aspect=\"auto\")\n",
    "plt.xticks([])\n",
    "plt.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748916f0",
   "metadata": {
    "id": "748916f0"
   },
   "source": [
    "<ol  start=\"4\"><li>Bilateral Filtering</li></ol>\n",
    "<i>cv.bilateralFilter(src, dest, sigmaColor, sigmaSpace[, dst[, borderType]])</i><br><br>\n",
    "\n",
    "cv.bilateralFilter() is highly effective in noise removal while keeping edges sharp. But the operation is slower compared to other filters. We already saw that a Gaussian filter takes the neighbourhood around the pixel and finds its Gaussian weighted average. This Gaussian filter is a function of space alone, that is, nearby pixels are considered while filtering. It doesn't consider whether pixels have almost the same intensity. It doesn't consider whether a pixel is an edge pixel or not. So it blurs the edges also, which we don't want to do.\n",
    "\n",
    "Bilateral filtering also takes a Gaussian filter in space, but one more Gaussian filter which is a function of pixel difference. The Gaussian function of space makes sure that only nearby pixels are considered for blurring, while the Gaussian function of intensity difference makes sure that only those pixels with similar intensities to the central pixel are considered for blurring. So it preserves the edges since pixels at edges will have large intensity variation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c948623e",
   "metadata": {
    "id": "c948623e",
    "outputId": "6970e3aa-bfaa-4cb8-821e-00789c63061c"
   },
   "outputs": [],
   "source": [
    "load_image(\"sources/wood.jpg\")\n",
    "blured = cv2.bilateralFilter(image, 50, 75, 75)\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title(\"real\")\n",
    "plt.imshow(convert(image), aspect=\"auto\")\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.subplot(122)\n",
    "plt.title(\"blured\")\n",
    "plt.imshow(convert(blured), aspect=\"auto\")\n",
    "plt.xticks([])\n",
    "plt.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7240ee",
   "metadata": {
    "id": "6e7240ee"
   },
   "source": [
    "<hr>\n",
    "<h3>Image Thresholding</h3>\n",
    "<i>cv2.threshold(source(gray), thresholdValue, maxVal, thresholdingTechnique)</i>\n",
    "<p>Thresholding is a technique in OpenCV, which is the assignment of pixel values in relation to the threshold value provided. In thresholding, each pixel value is compared with the threshold value. If the pixel value is smaller than the threshold, it is set to 0, otherwise, it is set to a maximum value (generally 255). Thresholding is a very popular segmentation technique, used for separating an object considered as a foreground from its background. A threshold is a value which has two regions on its either side i.e. below the threshold or above the threshold. \n",
    "In Computer Vision, this technique of thresholding is done on grayscale images. So initially, the image has to be converted in grayscale color space.</p>\n",
    "<br>\n",
    "<h4>Threshold Methods</h4>\n",
    "<ol>\n",
    "    <li>cv2.THRESH_BINARY: If pixel intensity is greater than the set threshold, value set to 255, else set to 0 (black).</li>\n",
    "    <li>cv2.THRESH_BINARY_INV: Inverted or Opposite case of cv2.THRESH_BINARY.</li>\n",
    "    <li>cv.THRESH_TRUNC: If pixel intensity value is greater than threshold, it is truncated to the threshold. The pixel values are set to be the same as the threshold. All other values remain the same.</li>\n",
    "    <li>cv.THRESH_TOZERO: Pixel intensity is set to 0, for all the pixels intensity, less than the threshold value.</li>\n",
    "    <li>cv.THRESH_TOZERO_INV: Inverted or Opposite case of cv2.THRESH_TOZERO.</li>\n",
    "</ol>\n",
    "\n",
    "<img src=\"sources/threshold-methods.png\" alt=\"threshold-methods.jpg is not available :(\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f42f0c9",
   "metadata": {
    "id": "8f42f0c9",
    "outputId": "cf25321e-ce8c-4eed-d756-6a60d0cb7c80"
   },
   "outputs": [],
   "source": [
    "load_image(\"sources/handwriting.jpg\")\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "thresh_bin = cv2.threshold(gray, 100, 255, cv2.THRESH_BINARY)\n",
    "thresh_bin_inv = cv2.threshold(gray, 100, 255, cv2.THRESH_BINARY_INV)\n",
    "thresh_trunc = cv2.threshold(gray, 100, 0, cv2.THRESH_TRUNC)\n",
    "thresh_to_0 = cv2.threshold(gray, 100, 0, cv2.THRESH_TOZERO)\n",
    "thresh_to_0_inv = cv2.threshold(gray, 100, 0, cv2.THRESH_TOZERO_INV)\n",
    "\n",
    "plt.subplot(151)\n",
    "plt.title(\"bin_thresh\")\n",
    "plt.imshow(thresh_bin[1], cmap=\"gray\")\n",
    "plt.xticks([]) \n",
    "plt.yticks([])\n",
    "\n",
    "plt.subplot(152)\n",
    "plt.title(\"bin_thresh_inv\")\n",
    "plt.imshow(thresh_bin_inv[1], cmap=\"gray\")\n",
    "plt.xticks([]) \n",
    "plt.yticks([])\n",
    "\n",
    "plt.subplot(153)\n",
    "plt.title(\"trunc_thresh\")\n",
    "plt.imshow(thresh_trunc[1], cmap=\"gray\")\n",
    "plt.xticks([]) \n",
    "plt.yticks([])\n",
    "\n",
    "plt.subplot(154)\n",
    "plt.title(\"to_0_thresh\")\n",
    "plt.imshow(thresh_to_0[1], cmap=\"gray\")\n",
    "plt.xticks([]) \n",
    "plt.yticks([])\n",
    "\n",
    "plt.subplot(155)\n",
    "plt.title(\"to_0_thresh_inv\")\n",
    "plt.imshow(thresh_to_0_inv[1], cmap=\"gray\")\n",
    "plt.xticks([]) \n",
    "plt.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b00178e",
   "metadata": {
    "id": "6b00178e"
   },
   "source": [
    "<hr>\n",
    "<h3>Contours</h3>\n",
    "<p>Contours are defined as the line joining all the points along the boundary of an image that are having the same intensity. Contours come handy in shape analysis, finding the size of the object of interest, and object detection.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3049dd12",
   "metadata": {
    "id": "3049dd12",
    "outputId": "fbd35b92-c5c0-4516-d44a-dc4655b8f33e"
   },
   "outputs": [],
   "source": [
    "load_image(\"sources/cars.png\")\n",
    "\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "edged = cv2.Canny(gray, 100, 250)\n",
    "contours, hierachy = cv2.findContours(edged, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "\n",
    "#Draw all contours(-1 means 'all contours')\n",
    "cv2.drawContours(image, contours, -1, (0, 255, 0), 2)\n",
    "\n",
    "plt.title(\"Total of {0} contours\".format(len(contours)))\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f173e967",
   "metadata": {
    "id": "f173e967"
   },
   "source": [
    "<h4>Contour approximation method</h4>\n",
    "<p>It is the third argument in the cv2.findCounter(). Above, we have described it to draw the boundary of the shape with same intensity. It stores the (x,y) coordinates of the boundary of a shape. But here the question arise does it store all the coordinates? That is specified by the contour approximation method.\n",
    "\n",
    "If we pass the cv.CHAIN_APPROX_NONE, it will store all the boundary points. Sometimes it does not need to store all the points coordinate, suppose we found the contours of a straight line where it does not require to store all the contour points, it requires only two endpoints to store. So for such case, we use cv.CHAIN_APPROX_NONE, it removes all redundant points and compresses the contours, thereby saving memory.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02b453a",
   "metadata": {
    "id": "a02b453a",
    "outputId": "37abe40c-8324-48a0-f67e-5c6f95ea2131"
   },
   "outputs": [],
   "source": [
    "bg = np.random.randint(255, 256, (200, 200), dtype=\"uint8\")\n",
    "bg_copy = bg.copy()\n",
    "mask = np.zeros((bg.shape[0], bg.shape[1]), dtype=\"uint8\")\n",
    "cv2.rectangle(mask, (30, 30), (80, 80), 255, 1)\n",
    "masked = cv2.bitwise_and(bg, bg, mask=mask)\n",
    "edged = cv2.Canny(masked, 100, 250)\n",
    "\n",
    "chain_approx_none_conts, _ = cv2.findContours(edged, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "chain_approx_simple_conts, _ = cv2.findContours(edged, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE) #much faster\n",
    "\n",
    "cv2.drawContours(bg, chain_approx_none_conts, -1, (0, 255, 0), 2)\n",
    "cv2.drawContours(bg_copy, chain_approx_simple_conts, -1, (0, 255, 0), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198cd0c6",
   "metadata": {
    "id": "198cd0c6",
    "outputId": "72fed66e-b519-4708-a63c-f4f635f536ca"
   },
   "outputs": [],
   "source": [
    "plt.subplot(121)\n",
    "plt.title(\"CHAIN_APPROX_NONE\")\n",
    "plt.imshow(bg, cmap=\"gray\")\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(\"CHAIN_APPROX_SIMPLE\\n(fast method)\")\n",
    "plt.imshow(bg_copy, cmap=\"gray\")\n",
    "plt.xticks([])\n",
    "plt.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90d29b9",
   "metadata": {
    "id": "b90d29b9"
   },
   "source": [
    "<hr>\n",
    "<h3>Template Matching</h3>\n",
    "<p>Template matching is a technique for finding areas of an image that are similar to a patch (template). \n",
    "A patch is a small image with certain features. The goal of template matching is to find the patch/template in an image. \n",
    "To find it, the user has to give two input images: Source Image (S) – The image to find the template in and Template Image (T) – The image that is to be found in the source image.</p>\n",
    "\n",
    "<p>It is basically a method for searching and finding the location of a template image in a larger image.\n",
    "The idea here is to find identical regions of an image that match a template we provide, giving a threshold.<br><br>\n",
    "The threshold depends on the accuracy with which we want to detect the template in the source image.\n",
    "For instance, if we are applying face recognition and we want to detect the eyes of a person, we can provide a random image of an eye as the template and search the source (the face of a person).\n",
    "In this case, since “eyes” show a large amount of variations from person to person, even if we set the threshold as 50%(0.5), the eye will be detected.\n",
    "In cases where almost identical templates are to be searched, the threshold should be set high.(t>=0.8)</p>\n",
    "\n",
    "<h4>How template matching works?</h4>\n",
    "<p>The template image simply slides over the input image (as in 2D convolution)\n",
    "The template and patch of input image under the template image are compared.\n",
    "The result obtained is compared with the threshold.\n",
    "If the result is greater than threshold, the portion will be marked as detected.\n",
    "In the function cv2.matchTemplate(img_gray,template,cv2.TM_CCOEFF_NORMED) the first parameter is the mainimage, second parameter is the template to be matched and third parameter is the method used for matching.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5b19df",
   "metadata": {
    "id": "8e5b19df",
    "outputId": "1c2e744b-6609-4a9c-dadc-b15c4b70948d"
   },
   "outputs": [],
   "source": [
    "load_image()\n",
    "grayed = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "template = cv2.imread(\"sources/template.png\", 0)\n",
    "w, h = template.shape\n",
    "\n",
    "result = cv2.matchTemplate(grayed, template, cv2.TM_CCOEFF_NORMED)\n",
    "threshold = 0.9\n",
    "loc = np.where(result >= threshold)\n",
    "\n",
    "for pt in zip(*loc[::-1]):\n",
    "    cv2.rectangle(image, pt, (pt[0] + w, pt[1] + h), (0, 255, 0), 0)\n",
    "\n",
    "plt.imshow(convert(image))   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20592b4",
   "metadata": {
    "id": "c20592b4"
   },
   "source": [
    "<h3>Limitations of Template Matching</h3>\n",
    "\n",
    "<ol>\n",
    "        <li>Pattern occurrences have to preserve the orientation of the reference pattern image(template)</li>\n",
    "    <li>As a result, it does not work for rotated or scaled versions of the template as a change in shape/size/shear etc. of object w.r.t. template will give a false match.</li>\n",
    "    <li>The method is inefficient when calculating the pattern correlation image for medium to large images as the process is time consuming.</li>\n",
    "</ol>\n",
    "\n",
    "<h3>Solution?</h3>\n",
    "\n",
    "<p>To avoid the issue caused by the different sizes of the template and original image we can use multiscaling. In case where, just because the dimensions of your template do not match the dimensions of the region in the image you want to match, does not mean that you cannot apply template matching.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814d5123",
   "metadata": {
    "id": "814d5123"
   },
   "source": [
    "<hr>\n",
    "<h3>Multiscaling</h3>\n",
    "\n",
    "<ol>\n",
    "    <li>Loop over the input image at multiple scales (i.e. make the input image progressively smaller and smaller).</li>\n",
    "    <li>Apply template matching using cv2.matchTemplate and keep track of the match with the largest correlation coefficient (along with the x, y-coordinates of the region with the largest correlation coefficient).</li>\n",
    "    <li>After looping over all scales, take the region with the largest correlation coefficient and use that as your “matched” region.</li>\n",
    "</ol>\n",
    "<a href=\"https://www.geeksforgeeks.org/template-matching-using-opencv-in-python/\"><i>for reference</i></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c98115b",
   "metadata": {
    "id": "9c98115b",
    "outputId": "d0e76fcb-06e9-418b-fdcc-84e02cef472b"
   },
   "outputs": [],
   "source": [
    "load_image()\n",
    "grayed = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "template = cv2.imread(\"sources/template.png\", 0)\n",
    "w, h = template.shape\n",
    "found = None\n",
    "\n",
    "for scale in np.linspace(0.2, 1.0, 20)[::-1]:\n",
    "    resized = np.resize(grayed, (grayed.shape[0], int(grayed.shape[1] * scale)))\n",
    "    result = cv2.matchTemplate(resized, template, cv2.TM_CCOEFF_NORMED)\n",
    "    (_, maxVal, _, maxLoc) = cv2.minMaxLoc(result)\n",
    "    \n",
    "    if found == None or found[0] < maxVal:\n",
    "        found = (maxVal, maxLoc, scale)\n",
    "    if resized.shape[0] < h or resized.shape[1] < w:\n",
    "        break\n",
    "        \n",
    "(_, maxLoc, r) = found\n",
    "(startX, startY) = int(maxLoc[0] * r), int(maxLoc[1] * r)\n",
    "(endX, endY) = int((maxLoc[0] + w) * r), int((maxLoc[1] + h) * r)\n",
    " \n",
    "cv2.rectangle(image, (startX, startY), (endX, endY), (0, 255, 0), 2)\n",
    "\n",
    "plt.imshow(convert(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de21f80",
   "metadata": {
    "id": "6de21f80"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2578c3fc",
   "metadata": {
    "id": "2578c3fc"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
